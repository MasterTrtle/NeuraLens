{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7b5870",
   "metadata": {},
   "source": [
    "## HF config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff00a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/code/NeuraLens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/medgemma/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "%cd /home/code/NeuraLens\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv(\".env\")\n",
    "# Access the HF_TOKEN environment variable\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c83edb",
   "metadata": {},
   "source": [
    "## Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b049e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 556 images in src/dataset/Motic-Human-tissues\n",
      "Creating HuggingFace datasets...\n",
      "Formatting training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6effcaeb1e54a6991f1aacc634a75b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de5180e92aa4317bcc2eba163d184f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assuming dataset is already downloaded in src/dataset\n",
    "from src.dataset import tissue_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset_path = \"src/dataset/Motic-Human-tissues\"\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Resize((512, 512)),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "transform = None\n",
    "batch_size = 1\n",
    "split_ratio = 0.8\n",
    "\n",
    "dataset = tissue_dataset(\n",
    "    images_dir_path=dataset_path,\n",
    "    transform=transform,\n",
    "    split_ratio=split_ratio,\n",
    "    batch_size=batch_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da621af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current type classes: ['human adrenal gland', 'human bone marrow', 'human cerebelum', 'human esophagus', 'human heart section', 'human hyaline cartilage', 'human kidney', 'human liver section', 'human salivary gland', 'human small intestine', 'human spleen', 'human stratified epithelium', 'human thymus', 'human tongue', 'human tonsil tongue']\n",
      "Current zoom classes: [4, 10, 20, 40]\n",
      "Current focus classes: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current type classes: {dataset.type_classes}\")\n",
    "print(f\"Current zoom classes: {dataset.zoom_classes}\")\n",
    "print(f\"Current focus classes: {dataset.focus_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33643d9d",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cffed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17c18734531498a9182d3fffede1454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "dataset.set_processor(processor)\n",
    "dataset.build_train_val_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a464655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c472f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add evaluate, and do it before & after training\n",
    "def evaluate(model, dataset):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c32968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/444 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 109/444 [02:19<07:04,  1.27s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# -- Training parameters --\n",
    "epochs = 3\n",
    "weight_decay = 0.001\n",
    "learning_rate = 2e-5\n",
    "scheduler_name = \"cosine\"\n",
    "ratio_warmup = 0.1\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "# Fix the dataloader reference\n",
    "scheduler = get_scheduler(\n",
    "    name=scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(len(dataset.train_loader) * epochs * ratio_warmup),\n",
    "    num_training_steps=len(dataset.train_loader) * epochs,\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0\n",
    "    \n",
    "    for batch in tqdm(dataset.train_loader, desc=\"Training\"):\n",
    "        # Convert messages to text format for the processor\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "    \n",
    "    avg_train_loss = train_loss / train_steps\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataset.val_loader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_steps\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
