{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7b5870",
   "metadata": {},
   "source": [
    "## HF config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff00a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/code/NeuraLens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/medgemma/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "%cd /home/code/NeuraLens\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv(\".env\")\n",
    "# Access the HF_TOKEN environment variable\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59865df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a59c4c64f0f4e11a9a8f88fe30acd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\n",
    "        \"GPU does not support bfloat16, please use a GPU that supports bfloat16.\"\n",
    "    )\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e19ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset as HFDataset, DatasetDict\n",
    "# from torch.utils.data import DataLoader\n",
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from typing import Any\n",
    "\n",
    "\n",
    "# # TODO add multiprocessing to speed up image loading\n",
    "# class tissue_dataset:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         images_dir_path=\"src/dataset/Motic-Human-tissues\",\n",
    "#         transform=None,\n",
    "#         split_ratio=0.8,\n",
    "#         train_batch_size=8,\n",
    "#         val_batch_size=8,\n",
    "#     ):\n",
    "#         \"\"\"Initializes the tissue dataset.\n",
    "#         Args:\n",
    "#             images_dir_path (str): Path to the directory containing tissue images.\n",
    "#             transform (callable, optional): A function/transform that takes in an image and returns a transformed version.\n",
    "#             split_ratio (float): The ratio of the dataset to be used for training.\n",
    "#             train_batch_size (int): The number of samples per batch for training.\n",
    "#             val_batch_size (int): The number of samples per batch for validation.\n",
    "#         \"\"\"\n",
    "#         self.split_ratio = split_ratio\n",
    "#         self.train_batch_size = train_batch_size\n",
    "#         self.val_batch_size = val_batch_size\n",
    "\n",
    "#         images = []\n",
    "#         focus = []\n",
    "#         type_labels = []\n",
    "#         zoom = []\n",
    "\n",
    "#         images_dir = Path(images_dir_path)\n",
    "#         for image in images_dir.glob(\"*.jpg\"):\n",
    "#             image_name = image.name.lower()\n",
    "#             if image_name.find(\"calibration\") == -1:\n",
    "#                 images.append(str(image))\n",
    "\n",
    "#                 if image_name.find(\"focus\") != -1:\n",
    "#                     focus.append(0)\n",
    "#                 else:\n",
    "#                     focus.append(1)\n",
    "\n",
    "#                 position = image_name.find(\"4x\")\n",
    "#                 if position != -1:\n",
    "#                     zoom.append(4)\n",
    "#                 else:\n",
    "#                     position = image_name.find(\"10x\")\n",
    "#                     if position != -1:\n",
    "#                         zoom.append(10)\n",
    "#                     else:\n",
    "#                         position = image_name.find(\"10x\")\n",
    "#                         if position != -1:\n",
    "#                             zoom.append(10)\n",
    "#                         else:\n",
    "#                             position = image_name.find(\"20x\")\n",
    "#                             if position != -1:\n",
    "#                                 zoom.append(20)\n",
    "#                             else:\n",
    "#                                 position = image_name.find(\"40x\")\n",
    "#                                 if position != -1:\n",
    "#                                     zoom.append(40)\n",
    "\n",
    "#                 type_labels.append(image_name[: position - 1].split(\"-\")[0])\n",
    "#         print(f\"Found {len(images)} images in {images_dir_path}\")\n",
    "\n",
    "#         if transform is not None:\n",
    "#             self.transform = transform\n",
    "#         else:\n",
    "#             self.transform = transforms.ToTensor()\n",
    "\n",
    "#         self.zoom_classes = sorted(list(set(zoom)))  # e.g., [4, 10, 20, 40]\n",
    "#         self.type_classes = sorted(\n",
    "#             list(set(type_labels))\n",
    "#         )  # e.g., ['adipose', 'blood', 'bone', etc.]\n",
    "#         self.focus_classes = [0, 1]  # Focused (0) or Unfocused (1)\n",
    "\n",
    "#         # Create mappings\n",
    "#         self.zoom_to_idx = {\n",
    "#             zoom_val: idx for idx, zoom_val in enumerate(self.zoom_classes)\n",
    "#         }\n",
    "#         self.type_to_idx = {\n",
    "#             type_name: idx for idx, type_name in enumerate(self.type_classes)\n",
    "#         }\n",
    "#         # Focused (0) or Unfocused (1)\n",
    "#         self.focus_to_idx = {\n",
    "#             \"focused\": 0,\n",
    "#             \"unfocused\": 1,\n",
    "#         }\n",
    "\n",
    "#         # Convert zoom and type to indices\n",
    "#         zoom_indices = [self.zoom_to_idx[z] for z in zoom]\n",
    "#         type_indices = [self.type_to_idx[t] for t in type_labels]\n",
    "\n",
    "#         # Split data before creating HuggingFace datasets\n",
    "#         self._split_data(images, focus, zoom_indices, type_indices)\n",
    "\n",
    "#         type_classes_mapping = {v: k for k, v in self.type_to_idx.items()}\n",
    "\n",
    "#         self.options = \"\\n\".join(\n",
    "#             [f\"{i}: {type_classes_mapping[i]}\" for i in type_classes_mapping]\n",
    "#         )\n",
    "\n",
    "#         self.PROMPT = f\"What is the most likely body part type shown in the histopathology image?\\n{self.options}\"\n",
    "\n",
    "#         # Create HuggingFace DatasetDict\n",
    "#         self._create_hf_datasets()\n",
    "\n",
    "#     def _split_data(self, images, focus, zoom_indices, type_indices):\n",
    "#         \"\"\"Split the data into train and validation sets.\"\"\"\n",
    "#         # Split the indices without stratification\n",
    "#         train_indices, val_indices = train_test_split(\n",
    "#             range(len(images)), test_size=1.0 - self.split_ratio, random_state=42\n",
    "#         )\n",
    "\n",
    "#         # Create train split\n",
    "#         self.train_data = {\n",
    "#             \"image_path\": [images[i] for i in train_indices],\n",
    "#             \"focus\": [focus[i] for i in train_indices],\n",
    "#             \"zoom\": [zoom_indices[i] for i in train_indices],\n",
    "#             \"type\": [type_indices[i] for i in train_indices],\n",
    "#         }\n",
    "\n",
    "#         # Create validation split\n",
    "#         self.val_data = {\n",
    "#             \"image_path\": [images[i] for i in val_indices],\n",
    "#             \"focus\": [focus[i] for i in val_indices],\n",
    "#             \"zoom\": [zoom_indices[i] for i in val_indices],\n",
    "#             \"type\": [type_indices[i] for i in val_indices],\n",
    "#         }\n",
    "\n",
    "#     def _create_hf_datasets(self):\n",
    "#         \"\"\"Create HuggingFace DatasetDict with train and validation splits.\"\"\"\n",
    "#         # Create individual datasets\n",
    "#         print(\"Creating HuggingFace datasets...\")\n",
    "#         train_dataset = HFDataset.from_dict(self.train_data)\n",
    "#         val_dataset = HFDataset.from_dict(self.val_data)\n",
    "\n",
    "#         # Apply message formatting for chat-based training\n",
    "#         print(\"Formatting training data...\")\n",
    "#         train_dataset = train_dataset.map(self._format_data, batched=False)\n",
    "\n",
    "#         print(\"Formatting validation data...\")\n",
    "#         val_dataset = val_dataset.map(self._format_test_data, batched=False)\n",
    "\n",
    "#         # Create DatasetDict\n",
    "#         self.dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "\n",
    "#     def build_train_val_loaders(self) -> None:\n",
    "#         \"\"\"Create DataLoaders for training and validation.\"\"\"\n",
    "#         self.train_dataset = self.dataset[\"train\"]\n",
    "#         self.val_dataset = self.dataset[\"validation\"]\n",
    "\n",
    "#         # Set format for PyTorch compatibility\n",
    "#         self.train_dataset.set_format(\"torch\")\n",
    "#         self.val_dataset.set_format(\"torch\")\n",
    "\n",
    "#         # Create DataLoaders\n",
    "#         self.train_loader = DataLoader(\n",
    "#             self.train_dataset,\n",
    "#             batch_size=self.train_batch_size,\n",
    "#             shuffle=True,\n",
    "#             collate_fn=self._collate_fn,\n",
    "#         )\n",
    "#         self.val_loader = DataLoader(\n",
    "#             self.val_dataset,\n",
    "#             batch_size=self.val_batch_size,\n",
    "#             collate_fn=self._collate_fn,\n",
    "#         )\n",
    "\n",
    "#     def _collate_fn(self, examples: list[dict[str, Any]]):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "#         for example in examples:\n",
    "#             image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "#             images.append(image)\n",
    "#             # print(f\"Image: {example['image_path']}, size: {image.size}\")\n",
    "#             text = self.processor.apply_chat_template(\n",
    "#                 example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "#             ).strip()\n",
    "#             # print(f\"Text: {text}, length: {len(text)}\")\n",
    "#             texts.append(text)\n",
    "\n",
    "#         # print(f\"Number of images: {len(images)}, Number of texts: {len(texts)}\")\n",
    "\n",
    "#         # Process each image-text pair individually and then batch them\n",
    "#         batched_inputs = []\n",
    "#         for i in range(len(images)):\n",
    "#             # Process one image-text pair at a time\n",
    "#             single_batch = self.processor(\n",
    "#                 text=[texts[i]], images=[images[i]], return_tensors=\"pt\", padding=True\n",
    "#             )\n",
    "#             batched_inputs.append(single_batch)\n",
    "\n",
    "#         # Now manually batch the results\n",
    "#         batch = {}\n",
    "#         for key in batched_inputs[0].keys():\n",
    "#             if key == \"pixel_values\":\n",
    "#                 # Stack image tensors\n",
    "#                 batch[key] = torch.cat([b[key] for b in batched_inputs], dim=0)\n",
    "#             else:\n",
    "#                 # Pad and stack text tensors\n",
    "#                 max_length = max(b[key].shape[1] for b in batched_inputs)\n",
    "#                 padded_tensors = []\n",
    "#                 for b in batched_inputs:\n",
    "#                     tensor = b[key]\n",
    "#                     if tensor.shape[1] < max_length:\n",
    "#                         # Pad with tokenizer's pad_token_id\n",
    "#                         padding = torch.full(\n",
    "#                             (tensor.shape[0], max_length - tensor.shape[1]),\n",
    "#                             self.processor.tokenizer.pad_token_id,\n",
    "#                             dtype=tensor.dtype,\n",
    "#                         )\n",
    "#                         tensor = torch.cat([tensor, padding], dim=1)\n",
    "#                     padded_tensors.append(tensor)\n",
    "#                 batch[key] = torch.cat(padded_tensors, dim=0)\n",
    "\n",
    "#         # The labels are the input_ids, with the padding and image tokens masked in\n",
    "#         # the loss computation\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "#         # Mask image tokens\n",
    "#         image_token_id = self.processor.tokenizer.convert_tokens_to_ids(\n",
    "#             self.processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "#         )\n",
    "\n",
    "#         # Mask tokens that are not used in the loss computation\n",
    "#         labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "#         labels[labels == image_token_id] = -100\n",
    "#         labels[labels == 262144] = -100\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "\n",
    "#     def set_processor(self, processor):\n",
    "#         \"\"\"Set the processor for the collate function.\"\"\"\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def _format_data(self, example: dict[str, Any]) -> dict[str, Any]:\n",
    "#         \"\"\"Format data for chat-based training.\"\"\"\n",
    "#         # Get the tissue type label and convert to class name\n",
    "#         tissue_type_idx = example[\"type\"]\n",
    "#         tissue_type_name = self.type_classes[tissue_type_idx]\n",
    "\n",
    "#         example[\"messages\"] = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": self.PROMPT,\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"assistant\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": f\"{tissue_type_idx}: {tissue_type_name}\",\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#         ]\n",
    "#         return example\n",
    "\n",
    "#     def _format_test_data(self, example: dict[str, Any]) -> dict[str, Any]:\n",
    "#         \"\"\"Format data for chat-based training.\"\"\"\n",
    "#         # Get the tissue type label and convert to class name\n",
    "\n",
    "#         example[\"messages\"] = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": self.PROMPT,\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#         ]\n",
    "#         return example\n",
    "\n",
    "#     def postprocess(\n",
    "#         self, prediction: list[dict[str, str]], do_full_match: bool = False\n",
    "#     ) -> int:\n",
    "#         response_text = prediction[0][\"generated_text\"]\n",
    "#         if do_full_match:\n",
    "#             # Try to extract the number directly from the response\n",
    "#             try:\n",
    "#                 return int(response_text.strip())\n",
    "#             except ValueError:\n",
    "#                 pass\n",
    "\n",
    "#         for i, label in enumerate(self.type_classes):\n",
    "#             # Search for `X: tissue type` or `(X) tissue type` in the response\n",
    "#             if (\n",
    "#                 f\"{i}: {label}\" in response_text\n",
    "#                 or f\"({i}) {label}\" in response_text\n",
    "#                 or label in response_text\n",
    "#             ):\n",
    "#                 return i\n",
    "#         return -1\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset[\"train\"]) + len(self.dataset[\"validation\"])\n",
    "\n",
    "#     def get_num_classes(self):\n",
    "#         \"\"\"Returns the number of classes for each label type.\"\"\"\n",
    "#         return {\n",
    "#             \"focus\": 2,  # focused (0) or unfocused (1)\n",
    "#             \"zoom\": len(self.zoom_classes),\n",
    "#             \"type\": len(self.type_classes),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08adf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "# TODO add multiprocessing to speed up image loading\n",
    "class tissue_dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir_path=\"src/dataset/Motic-Human-tissues\",\n",
    "        transform=None,\n",
    "        split_ratio=0.8,\n",
    "        train_batch_size=8,\n",
    "        val_batch_size=8,\n",
    "    ):\n",
    "        \"\"\"Initializes the tissue dataset.\n",
    "        Args:\n",
    "            images_dir_path (str): Path to the directory containing tissue images.\n",
    "            transform (callable, optional): A function/transform that takes in an image and returns a transformed version.\n",
    "            split_ratio (float): The ratio of the dataset to be used for training.\n",
    "            train_batch_size (int): The number of samples per batch for training.\n",
    "            val_batch_size (int): The number of samples per batch for validation.\n",
    "        \"\"\"\n",
    "        self.split_ratio = split_ratio\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "\n",
    "        images = []\n",
    "        focus = []\n",
    "        type_labels = []\n",
    "        zoom = []\n",
    "\n",
    "        images_dir = Path(images_dir_path)\n",
    "        for image in images_dir.glob(\"*.jpg\"):\n",
    "            image_name = image.name.lower()\n",
    "            if image_name.find(\"calibration\") == -1:\n",
    "                images.append(str(image))\n",
    "\n",
    "                if image_name.find(\"focus\") != -1:\n",
    "                    focus.append(0)\n",
    "                else:\n",
    "                    focus.append(1)\n",
    "\n",
    "                position = image_name.find(\"4x\")\n",
    "                if position != -1:\n",
    "                    zoom.append(4)\n",
    "                else:\n",
    "                    position = image_name.find(\"10x\")\n",
    "                    if position != -1:\n",
    "                        zoom.append(10)\n",
    "                    else:\n",
    "                        position = image_name.find(\"10x\")\n",
    "                        if position != -1:\n",
    "                            zoom.append(10)\n",
    "                        else:\n",
    "                            position = image_name.find(\"20x\")\n",
    "                            if position != -1:\n",
    "                                zoom.append(20)\n",
    "                            else:\n",
    "                                position = image_name.find(\"40x\")\n",
    "                                if position != -1:\n",
    "                                    zoom.append(40)\n",
    "\n",
    "                type_labels.append(image_name[: position - 1].split(\"-\")[0])\n",
    "        print(f\"Found {len(images)} images in {images_dir_path}\")\n",
    "\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.ToTensor()\n",
    "\n",
    "        self.zoom_classes = sorted(list(set(zoom)))  # e.g., [4, 10, 20, 40]\n",
    "        self.type_classes = sorted(\n",
    "            list(set(type_labels))\n",
    "        )  # e.g., ['adipose', 'blood', 'bone', etc.]\n",
    "        self.focus_classes = [0, 1]  # Focused (0) or Unfocused (1)\n",
    "\n",
    "        # Create mappings\n",
    "        self.zoom_to_idx = {\n",
    "            zoom_val: idx for idx, zoom_val in enumerate(self.zoom_classes)\n",
    "        }\n",
    "        self.type_to_idx = {\n",
    "            type_name: idx for idx, type_name in enumerate(self.type_classes)\n",
    "        }\n",
    "\n",
    "        # Convert zoom and type to indices\n",
    "        zoom_indices = [self.zoom_to_idx[z] for z in zoom]\n",
    "        type_indices = [self.type_to_idx[t] for t in type_labels]\n",
    "\n",
    "        # Split data before creating HuggingFace datasets\n",
    "        self._split_data(images, focus, zoom_indices, type_indices)\n",
    "\n",
    "        type_classes_mapping = {v: k for k, v in self.type_to_idx.items()}\n",
    "        zoom_classes_mapping = {v: k for k, v in self.zoom_to_idx.items()}\n",
    "        focus_classes_mapping = {0: \"focused\", 1: \"unfocused\"}\n",
    "\n",
    "        self.type_options = \"\\n\".join(\n",
    "            [f\"{i}: {type_classes_mapping[i]}\" for i in type_classes_mapping]\n",
    "        )\n",
    "        self.zoom_options = \"\\n\".join(\n",
    "            [f\"{i}: {zoom_classes_mapping[i]}x\" for i in zoom_classes_mapping]\n",
    "        )\n",
    "        self.focus_options = \"\\n\".join(\n",
    "            [f\"{i}: {focus_classes_mapping[i]}\" for i in focus_classes_mapping]\n",
    "        )\n",
    "\n",
    "        self.PROMPT = f\"\"\"Analyze this histopathology image and provide the following information:\n",
    "\n",
    "        Tissue Type:\n",
    "        {self.type_options}\n",
    "\n",
    "        Zoom Level:\n",
    "        {self.zoom_options}\n",
    "\n",
    "        Focus Quality:\n",
    "        {self.focus_options}\n",
    "\n",
    "        Please respond in the following JSON format:\n",
    "        {{\n",
    "        \"tissue_type\": \"X: tissue_name\",\n",
    "        \"zoom_level\": \"Y: Zx\",\n",
    "        \"focus_quality\": \"Z: focus_status\"\n",
    "        }}\"\"\"\n",
    "        # Create HuggingFace DatasetDict\n",
    "        self._create_hf_datasets()\n",
    "\n",
    "    def _split_data(self, images, focus, zoom_indices, type_indices):\n",
    "        \"\"\"Split the data into train and validation sets.\"\"\"\n",
    "        # Split the indices without stratification\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(images)), test_size=1.0 - self.split_ratio, random_state=42\n",
    "        )\n",
    "\n",
    "        # Create train split\n",
    "        self.train_data = {\n",
    "            \"image_path\": [images[i] for i in train_indices],\n",
    "            \"focus\": [focus[i] for i in train_indices],\n",
    "            \"zoom\": [zoom_indices[i] for i in train_indices],\n",
    "            \"type\": [type_indices[i] for i in train_indices],\n",
    "        }\n",
    "\n",
    "        # Create validation split\n",
    "        self.val_data = {\n",
    "            \"image_path\": [images[i] for i in val_indices],\n",
    "            \"focus\": [focus[i] for i in val_indices],\n",
    "            \"zoom\": [zoom_indices[i] for i in val_indices],\n",
    "            \"type\": [type_indices[i] for i in val_indices],\n",
    "        }\n",
    "\n",
    "    def _create_hf_datasets(self):\n",
    "        \"\"\"Create HuggingFace DatasetDict with train and validation splits.\"\"\"\n",
    "        # Create individual datasets\n",
    "        print(\"Creating HuggingFace datasets...\")\n",
    "        train_dataset = HFDataset.from_dict(self.train_data)\n",
    "        val_dataset = HFDataset.from_dict(self.val_data)\n",
    "\n",
    "        # Apply message formatting for chat-based training\n",
    "        print(\"Formatting training data...\")\n",
    "        train_dataset = train_dataset.map(self._format_data, batched=False)\n",
    "\n",
    "        print(\"Formatting validation data...\")\n",
    "        val_dataset = val_dataset.map(self._format_test_data, batched=False)\n",
    "\n",
    "        # Create DatasetDict\n",
    "        self.dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "\n",
    "    def build_train_val_loaders(self) -> None:\n",
    "        \"\"\"Create DataLoaders for training and validation.\"\"\"\n",
    "        self.train_dataset = self.dataset[\"train\"]\n",
    "        self.val_dataset = self.dataset[\"validation\"]\n",
    "\n",
    "        # Set format for PyTorch compatibility\n",
    "        self.train_dataset.set_format(\"torch\")\n",
    "        self.val_dataset.set_format(\"torch\")\n",
    "\n",
    "        # Create DataLoaders\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            collate_fn=self._collate_fn,\n",
    "        )\n",
    "\n",
    "    def _collate_fn(self, examples: list[dict[str, Any]]):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "            # print(f\"Image: {example['image_path']}, size: {image.size}\")\n",
    "            text = self.processor.apply_chat_template(\n",
    "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "            ).strip()\n",
    "            # print(f\"Text: {text}, length: {len(text)}\")\n",
    "            texts.append(text)\n",
    "\n",
    "        # print(f\"Number of images: {len(images)}, Number of texts: {len(texts)}\")\n",
    "\n",
    "        # Process each image-text pair individually and then batch them\n",
    "        batched_inputs = []\n",
    "        for i in range(len(images)):\n",
    "            # Process one image-text pair at a time\n",
    "            single_batch = self.processor(\n",
    "                text=[texts[i]], images=[images[i]], return_tensors=\"pt\", padding=True\n",
    "            )\n",
    "            batched_inputs.append(single_batch)\n",
    "\n",
    "        # Now manually batch the results\n",
    "        batch = {}\n",
    "        for key in batched_inputs[0].keys():\n",
    "            if key == \"pixel_values\":\n",
    "                # Stack image tensors\n",
    "                batch[key] = torch.cat([b[key] for b in batched_inputs], dim=0)\n",
    "            else:\n",
    "                # Pad and stack text tensors\n",
    "                max_length = max(b[key].shape[1] for b in batched_inputs)\n",
    "                padded_tensors = []\n",
    "                for b in batched_inputs:\n",
    "                    tensor = b[key]\n",
    "                    if tensor.shape[1] < max_length:\n",
    "                        # Pad with tokenizer's pad_token_id\n",
    "                        padding = torch.full(\n",
    "                            (tensor.shape[0], max_length - tensor.shape[1]),\n",
    "                            self.processor.tokenizer.pad_token_id,\n",
    "                            dtype=tensor.dtype,\n",
    "                        )\n",
    "                        tensor = torch.cat([tensor, padding], dim=1)\n",
    "                    padded_tensors.append(tensor)\n",
    "                batch[key] = torch.cat(padded_tensors, dim=0)\n",
    "\n",
    "        # The labels are the input_ids, with the padding and image tokens masked in\n",
    "        # the loss computation\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # Mask image tokens\n",
    "        image_token_id = self.processor.tokenizer.convert_tokens_to_ids(\n",
    "            self.processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "\n",
    "        # Mask tokens that are not used in the loss computation\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        labels[labels == image_token_id] = -100\n",
    "        labels[labels == 262144] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def set_processor(self, processor):\n",
    "        \"\"\"Set the processor for the collate function.\"\"\"\n",
    "        self.processor = processor\n",
    "\n",
    "    def _format_data(self, example: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Format data for chat-based training.\"\"\"\n",
    "        tissue_type_idx = example[\"type\"]\n",
    "        tissue_type_name = self.type_classes[tissue_type_idx]\n",
    "        zoom_idx = example[\"zoom\"]\n",
    "        zoom_value = self.zoom_classes[zoom_idx]\n",
    "        focus_idx = example[\"focus\"]\n",
    "        focus_status = \"focused\" if focus_idx == 0 else \"unfocused\"\n",
    "\n",
    "        # Randomly choose between JSON and natural language responses (70% JSON, 30% natural)\n",
    "        import random\n",
    "\n",
    "        use_json = random.random() < 0.7\n",
    "\n",
    "        if use_json:\n",
    "            response_text = f\"\"\"{{\n",
    "    \"tissue_type\": \"{tissue_type_idx}: {tissue_type_name}\",\n",
    "    \"zoom_level\": \"{zoom_idx}: {zoom_value}x\",\n",
    "    \"focus_quality\": \"{focus_idx}: {focus_status}\"\n",
    "    }}\"\"\"\n",
    "        else:\n",
    "            response_text = f\"This histopathology image shows {tissue_type_name} tissue at {zoom_value}x magnification. The image appears to be {focus_status}.\"\n",
    "\n",
    "        example[\"messages\"] = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": self.PROMPT},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": response_text},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        return example\n",
    "\n",
    "    def _format_test_data(self, example: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Format data for chat-based training.\"\"\"\n",
    "        example[\"messages\"] = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": self.PROMPT,\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        return example\n",
    "\n",
    "    def postprocess(\n",
    "        self, prediction: list[dict[str, str]], do_full_match: bool = False\n",
    "    ) -> dict:\n",
    "        response_text = prediction[0][\"generated_text\"]\n",
    "\n",
    "        # Clean the response text first\n",
    "        response_text = response_text.strip()\n",
    "\n",
    "        # Try to extract the first JSON block\n",
    "        import re\n",
    "        import json\n",
    "\n",
    "        # Look for JSON pattern\n",
    "        json_pattern = r\"\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}\"\n",
    "        json_matches = re.findall(json_pattern, response_text)\n",
    "\n",
    "        for json_match in json_matches:\n",
    "            try:\n",
    "                parsed = json.loads(json_match)\n",
    "                if all(\n",
    "                    key in parsed\n",
    "                    for key in [\"tissue_type\", \"zoom_level\", \"focus_quality\"]\n",
    "                ):\n",
    "                    return {\n",
    "                        \"tissue_type\": self._extract_index(\n",
    "                            parsed.get(\"tissue_type\", \"\")\n",
    "                        ),\n",
    "                        \"zoom_level\": self._extract_index(parsed.get(\"zoom_level\", \"\")),\n",
    "                        \"focus_quality\": self._extract_index(\n",
    "                            parsed.get(\"focus_quality\", \"\")\n",
    "                        ),\n",
    "                    }\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                continue\n",
    "\n",
    "        # Fallback to text parsing if no valid JSON found\n",
    "        return {\n",
    "            \"tissue_type\": self._extract_tissue_from_text(response_text),\n",
    "            \"zoom_level\": self._extract_zoom_from_text(response_text),\n",
    "            \"focus_quality\": self._extract_focus_from_text(response_text),\n",
    "        }\n",
    "\n",
    "    def _extract_index(self, text: str) -> int:\n",
    "        \"\"\"Extract index from 'X: value' format\"\"\"\n",
    "        try:\n",
    "            return int(text.split(\":\")[0].strip())\n",
    "        except (ValueError, IndexError):\n",
    "            return -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"train\"]) + len(self.dataset[\"validation\"])\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        \"\"\"Returns the number of classes for each label type.\"\"\"\n",
    "        return {\n",
    "            \"focus\": 2,  # focused (0) or unfocused (1)\n",
    "            \"zoom\": len(self.zoom_classes),\n",
    "            \"type\": len(self.type_classes),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c83edb",
   "metadata": {},
   "source": [
    "## Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b049e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 556 images in src/dataset/Motic-Human-tissues\n",
      "Creating HuggingFace datasets...\n",
      "Formatting training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae49e47dd964607be664aaa962ae440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40392bf6fff8441ba268cf361d7b1245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assuming dataset is already downloaded in src/dataset\n",
    "# from src.dataset import tissue_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset_path = \"src/dataset/Motic-Human-tissues\"\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Resize((512, 512)),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "transform = None\n",
    "\n",
    "train_batch_size = 1\n",
    "val_batch_size = 8\n",
    "split_ratio = 0.8\n",
    "\n",
    "dataset = tissue_dataset(\n",
    "    images_dir_path=dataset_path,\n",
    "    transform=transform,\n",
    "    split_ratio=split_ratio,\n",
    "    train_batch_size=train_batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da621af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current type classes: ['human adrenal gland', 'human bone marrow', 'human cerebelum', 'human esophagus', 'human heart section', 'human hyaline cartilage', 'human kidney', 'human liver section', 'human salivary gland', 'human small intestine', 'human spleen', 'human stratified epithelium', 'human thymus', 'human tongue', 'human tonsil tongue']\n",
      "Current zoom classes: [4, 10, 20, 40]\n",
      "Current focus classes: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current type classes: {dataset.type_classes}\")\n",
    "print(f\"Current zoom classes: {dataset.zoom_classes}\")\n",
    "print(f\"Current focus classes: {dataset.focus_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4f9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_processor(processor)\n",
    "dataset.build_train_val_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5357e4a",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b8477",
   "metadata": {},
   "source": [
    "## inference on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf3914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics_multitask(\n",
    "    predictions: list[dict], references: dict\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute metrics for all three tasks: tissue_type, zoom_level, focus_quality\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Extract predictions for each task\n",
    "    tissue_predictions = [pred.get(\"tissue_type\", -1) for pred in predictions]\n",
    "    zoom_predictions = [pred.get(\"zoom_level\", -1) for pred in predictions]\n",
    "    focus_predictions = [pred.get(\"focus_quality\", -1) for pred in predictions]\n",
    "\n",
    "    # Get references for each task\n",
    "    tissue_references = references[\"tissue_type\"]\n",
    "    zoom_references = references[\"zoom_level\"]\n",
    "    focus_references = references[\"focus_quality\"]\n",
    "\n",
    "    # Compute metrics for tissue type\n",
    "    tissue_acc = accuracy_metric.compute(\n",
    "        predictions=tissue_predictions, references=tissue_references\n",
    "    )\n",
    "    tissue_f1 = f1_metric.compute(\n",
    "        predictions=tissue_predictions, references=tissue_references, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # Compute metrics for zoom level\n",
    "    zoom_acc = accuracy_metric.compute(\n",
    "        predictions=zoom_predictions, references=zoom_references\n",
    "    )\n",
    "    zoom_f1 = f1_metric.compute(\n",
    "        predictions=zoom_predictions, references=zoom_references, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # Compute metrics for focus quality\n",
    "    focus_acc = accuracy_metric.compute(\n",
    "        predictions=focus_predictions, references=focus_references\n",
    "    )\n",
    "    focus_f1 = f1_metric.compute(\n",
    "        predictions=focus_predictions, references=focus_references, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # Organize metrics by task\n",
    "    metrics.update(\n",
    "        {\n",
    "            \"tissue_type_accuracy\": tissue_acc[\"accuracy\"],\n",
    "            \"tissue_type_f1\": tissue_f1[\"f1\"],\n",
    "            \"zoom_level_accuracy\": zoom_acc[\"accuracy\"],\n",
    "            \"zoom_level_f1\": zoom_f1[\"f1\"],\n",
    "            \"focus_quality_accuracy\": focus_acc[\"accuracy\"],\n",
    "            \"focus_quality_f1\": focus_f1[\"f1\"],\n",
    "            # Overall averages\n",
    "            \"overall_accuracy\": (\n",
    "                tissue_acc[\"accuracy\"] + zoom_acc[\"accuracy\"] + focus_acc[\"accuracy\"]\n",
    "            )\n",
    "            / 3,\n",
    "            \"overall_f1\": (tissue_f1[\"f1\"] + zoom_f1[\"f1\"] + focus_f1[\"f1\"]) / 3,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    pt_outputs = []\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Collect ground truth references for all tasks\n",
    "    references = {\n",
    "        \"tissue_type\": dataset.val_dataset[\"type\"],\n",
    "        \"zoom_level\": dataset.val_dataset[\"zoom\"],\n",
    "        \"focus_quality\": dataset.val_dataset[\"focus\"],\n",
    "    }\n",
    "\n",
    "    for batch in tqdm(dataset.val_loader, desc=\"Processing validation batches\"):\n",
    "        # Move batch to the same device as the model\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Get actual batch size (number of samples in this batch)\n",
    "        actual_batch_size = batch[\"input_ids\"].shape[0]\n",
    "\n",
    "        # Generate predictions for the entire batch\n",
    "        outputs = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_new_tokens=200,  # Increased for JSON responses\n",
    "            do_sample=False,  # Set to True for sampling, False for greedy decoding\n",
    "            # Add stopping criteria\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "        # Decode only the newly generated tokens (exclude the input)\n",
    "        input_length = batch[\"input_ids\"].shape[1]\n",
    "        generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = processor.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Process each sample in the current batch\n",
    "        for i in range(actual_batch_size):\n",
    "            # Format as expected by postprocess method\n",
    "            formatted_output = {\"generated_text\": generated_text[i]}\n",
    "            pt_outputs.append(formatted_output)\n",
    "\n",
    "        batch_idx += 1\n",
    "        # print the first generated output of the batch\n",
    "        print(\n",
    "            f\"Batch {batch_idx} processed, first output: {pt_outputs[-1]['generated_text']}\"\n",
    "        )\n",
    "        print(\"-\" * 50)\n",
    "    # Process predictions - now returns dict for each prediction\n",
    "    pt_predictions = [dataset.postprocess([out]) for out in pt_outputs]\n",
    "\n",
    "    print(f\"\\nSample predictions: {pt_predictions[:3]}\")  # Show first 3 predictions\n",
    "    print(f\"Total predictions: {len(pt_predictions)}\")\n",
    "\n",
    "    # Compute metrics for all tasks\n",
    "    pt_metrics = compute_metrics_multitask(pt_predictions, references)\n",
    "\n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(\n",
    "        f\"Tissue Type - Accuracy: {pt_metrics['tissue_type_accuracy']:.3f}, F1: {pt_metrics['tissue_type_f1']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Zoom Level - Accuracy: {pt_metrics['zoom_level_accuracy']:.3f}, F1: {pt_metrics['zoom_level_f1']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Focus Quality - Accuracy: {pt_metrics['focus_quality_accuracy']:.3f}, F1: {pt_metrics['focus_quality_f1']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Overall - Accuracy: {pt_metrics['overall_accuracy']:.3f}, F1: {pt_metrics['overall_f1']:.3f}\"\n",
    "    )\n",
    "\n",
    "    return pt_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0229a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation batches:   0%|          | 0/14 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:   7%|▋         | 1/14 [00:26<05:42, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed, first output: \n",
      "\n",
      "Based on the provided image, here's the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"tissue_type\": \"9: human small intestine\",\n",
      "  \"zoom_level\": \"2: 20x\",\n",
      "  \"focus_quality\": \"0: focused\"\n",
      "}\n",
      "```\n",
      "Final Answer: The final answer is $\\boxed{{tissue_type: \"9: human small intestine\", zoom_level: \"2: 20x\", focus_quality: \"0: focused\"}} $\n",
      "```json\n",
      "{\n",
      "  \"tissue_type\": \"9: human small intestine\",\n",
      "  \"zoom_level\": \"2: 20x\",\n",
      "  \"focus_quality\": \"0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:   7%|▋         | 1/14 [00:48<10:35, 48.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_line_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 91\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     88\u001b[0m actual_batch_size \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Generate predictions for the entire batch\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased for JSON responses\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True for sampling, False for greedy decoding\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Add stopping criteria\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Decode only the newly generated tokens (exclude the input)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m input_length \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2640\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/generation/utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3611\u001b[0m     outputs,\n\u001b[1;32m   3612\u001b[0m     model_kwargs,\n\u001b[1;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3614\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:1083\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1079\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1080\u001b[0m )\n\u001b[1;32m   1081\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:937\u001b[0m, in \u001b[0;36mGemma3Model.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **lm_kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     causal_mask_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_causal_mask(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs),\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_sliding_window_causal_mask(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs),\n\u001b[1;32m    935\u001b[0m     }\n\u001b[0;32m--> 937\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ModelOutputWithPast(\n\u001b[1;32m    951\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[1;32m    952\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m     image_hidden_states\u001b[38;5;241m=\u001b[39mimage_features \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    956\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:565\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    563\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 565\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:385\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[0;32m--> 385\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    397\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:311\u001b[0m, in \u001b[0;36mGemma3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    309\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 311\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    312\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    313\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:490\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    486\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    488\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:393\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m bind_default_args(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mforward, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43m_functorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_dead_wrappers\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/_functorch/utils.py:34\u001b[0m, in \u001b[0;36munwrap_dead_wrappers\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munwrap_dead_wrappers\u001b[39m(args: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# NB: doesn't use tree_map_only for performance reasons\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43munwrap_if_dead\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/medgemma/lib/python3.10/site-packages/torch/_functorch/utils.py:35\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munwrap_dead_wrappers\u001b[39m(args: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# NB: doesn't use tree_map_only for performance reasons\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 35\u001b[0m         unwrap_if_dead(arg) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_line_metrics = evaluate(base_model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33643d9d",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a464655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    # target_modules=\"all-linear\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],  # Target attention layers only\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d829724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,354,403,840 || all params: 5,654,483,312 || trainable%: 23.9527\n",
      "trainable parameters: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"trainable parameters: {model.print_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5865362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor image size: {'height': 896, 'width': 896}\n",
      "Image processor config: Gemma3ImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pan_and_scan\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"Gemma3ImageProcessor\",\n",
      "  \"image_seq_length\": 256,\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"pan_and_scan_max_num_crops\": null,\n",
      "  \"pan_and_scan_min_crop_size\": null,\n",
      "  \"pan_and_scan_min_ratio_to_activate\": null,\n",
      "  \"processor_class\": \"Gemma3Processor\",\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 896,\n",
      "    \"width\": 896\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this after loading the processor to check the image size\n",
    "print(f\"Processor image size: {processor.image_processor.size}\")\n",
    "print(f\"Image processor config: {processor.image_processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c32968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/444 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 444/444 [08:41<00:00,  1.17s/it, loss=0.0260, win_acc=0.010, lr=1.69e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6139\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 155/444 [03:00<05:36,  1.17s/it, loss=0.0137, win_acc=0.010, lr=1.34e-05]"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "\n",
    "# TODO remove window accuracy\n",
    "# TODO check if weighted avg\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# -- Training parameters --\n",
    "epochs = 3  # 1 for testing, 3 for full training\n",
    "weight_decay = 0.001\n",
    "learning_rate = 2e-5\n",
    "scheduler_name = \"cosine\"\n",
    "ratio_warmup = 0.1\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(len(dataset.train_loader) * epochs * ratio_warmup),\n",
    "    num_training_steps=len(dataset.train_loader) * epochs,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0\n",
    "    windows_acc = collections.deque(maxlen=5)  # Initialize the windowed accuracy queue\n",
    "    pbar = tqdm(dataset.train_loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "\n",
    "        # Convert messages to text format for the processor\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Only compute accuracy on non-masked labels\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        valid_labels = batch[\"labels\"] != -100\n",
    "        if valid_labels.sum() > 0:\n",
    "            batch_accuracy = (\n",
    "                (predictions[valid_labels] == batch[\"labels\"][valid_labels])\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "        else:\n",
    "            batch_accuracy = 0.0\n",
    "\n",
    "        # Update the windowed accuracy\n",
    "        windows_acc.append(batch_accuracy)\n",
    "        current_windowed_acc = np.mean(windows_acc)\n",
    "\n",
    "        # Use the model's outputs to compute the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "\n",
    "        # Update progress bar with windowed accuracy\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"win_acc\": f\"{current_windowed_acc:.3f}\",\n",
    "                \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    avg_train_loss = train_loss / train_steps\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "from time import strftime\n",
    "\n",
    "save_path = f\"src/models/medgemma-4b-it-lora-{strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60343a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05227a9ccffa4869acbdcd23173aa274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load the base model with the same configuration as during training\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6556bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation batches:   0%|          | 0/14 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:   7%|▋         | 1/14 [00:04<01:03,  4.89s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  14%|█▍        | 2/14 [00:09<00:59,  4.99s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  21%|██▏       | 3/14 [00:14<00:54,  4.97s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  29%|██▊       | 4/14 [00:19<00:49,  4.93s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  36%|███▌      | 5/14 [00:24<00:44,  4.91s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  43%|████▎     | 6/14 [00:29<00:39,  4.95s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  50%|█████     | 7/14 [00:34<00:34,  4.99s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  57%|█████▋    | 8/14 [00:39<00:29,  4.95s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  64%|██████▍   | 9/14 [00:44<00:24,  4.97s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  71%|███████▏  | 10/14 [00:49<00:20,  5.00s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  79%|███████▊  | 11/14 [00:54<00:14,  4.98s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  86%|████████▌ | 12/14 [00:59<00:09,  4.96s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches:  93%|█████████▎| 13/14 [01:04<00:04,  5.00s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "Processing validation batches: 100%|██████████| 14/14 [01:09<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All predictions: [10, 7, 13, 13, 7, 10, 7, 9, 4, 5, 13, 8, 13, 5, 10, 9, 9, 10, 0, 0, 13, 13, 8, 10, 0, 0, 0, 10, 8, 9, 13, 10, 8, 8, 10, 7, 10, 0, 13, 13, 0, 7, 9, 9, 10, 8, 11, 10, 4, 8, 0, 8, 10, 7, 5, 13, 10, 0, 9, 9, 7, 13, 13, 0, 10, 9, 0, 0, 9, 8, 8, 8, 5, 5, 5, 10, 8, 10, 13, 8, 0, 13, 13, 10, 9, 10, 13, 13, 10, 9, 7, 10, 10, 10, 7, 10, 10, 5, 10, 9, 5, 8, 10, 10, 10, 11, 9, 10, 0, 4, 10, 13]\n",
      "Metrics: {'accuracy': 0.6785714285714286, 'f1': 0.6322135264362155}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_metrics = evaluate(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
